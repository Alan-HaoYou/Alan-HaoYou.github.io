---
layout: post
title: ACL 2017 - A readthrough
thumbnail: "/img/og_sigir2017.png" 
---

[ACL 2017](http://acl2017.org/) with its nearly 200 full papers and more than 100 short papers was a treasure trove again for inspiring approaches, problems and datasets. There is really only one topic I stay away from and that is parsing - it just does not hold a lot of interest for me. The vast majority of papers contain a strong empirical component and it takes a while to actually find papers whose baselines come from papers written in 2013 or earlier; in fact, many papers resort to just reporting baselines from 2016 and 2017 papers. This is quite a contrast to IR, where the deep neural IR models are routinely compared to classic - and robust - retrieval models from the 1980s and 1990s. Are most of the NLP tasks new or did deep learning kill every single 'classic' baseline? 

Here are the papers I found most interesting this year:

* [*What do Neural Machine Translation Models Learn about Morphology?*](http://aclweb.org/anthology/P/P17/P17-1080.pdf) is a must-read for anyone who is interested in **analyzing the deep net black box**. The authors take up the challenge for neural machine translation models and explore the impact of several neural design decisions. There should be a paper like this for every model type and task (imho).

* [*FOIL it! Find One mismatch between Image and Language caption*](http://aclweb.org/anthology/P/P17/P17-1024.pdf) takes a closer look at visual question answering (VQA: given an image an a question, generate an answer) and image captioning (IC: given an image, generate a caption) approaches by creating a **diagnostic dataset** based on the [MS-COCO](http://mscoco.org/) benchmark. MS-COCO contains 300K images, each with five(+) different captions, written by crowd workers. VQA and IC approaches achieve great results on this benchmark, but do they truly learn how to answer and caption or do they exploit biases in the dataset? This diagnostic dataset provides more than a few hints that it is actually the latter. The authors take the MS-COCO image/caption pairs as their starting point and introduce a single error (a semantically related but incorrect term) into each caption. They then adapt state-of-the-art IC and VQA approaches for the task of caption classification (correct/foil). These state-of-the-art algorithms are not doing too well at detecting foil captions (accuracy of 45.44 compared to 94.52 that a majority vote of humans achieves), indicating that there is still a long way to go.

* [*A Syntactic Neural Model for General-Purpose Code Generation*](http://aclweb.org/anthology/P/P17/P17-1041.pdf) considers *"... the problem of **parsing natural language descriptions into source code** written in a general-purpose programming language like Python... Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge."* Sounds like the dream of every first year student who hates his/her programming class. A description of the problem goes into the system and runnable Python code comes out. Scary to think about as a programming class instructor. The paper shows that this problem cannot be solved with off-the-shelf DL approaches, quite a lot of domain adaptation is required to make it work. The results show that meaningful code generation is possible: <br><img src="../img/P17-1041.png" width="350px"><br> with the caveat that at the moment the input descriptions are short and concise, such as this one from the DJANGO dataset: `join app config.path and string ’locale’ into a file path, substitute it for localedir` which the code generator is able to correctly translate into Python: `localedir = os.path.join(app config.path, ’locale’)`.


* [*Get To The Point: Summarization with Pointer-Generator Networks*](http://aclweb.org/anthology/P/P17/P17-1099.pdf) tackles the two issues that have plagued abstractive text summarization (a topic that also is of importance in IR): repetition of content and inaccurate summarization of facts. A paper very focused on the weaknesses of existing methods and how to resolve them in a meaningful way.

* [*Affect-LM: A Neural Language Model for Customizable Affective Text Generation*](http://aclweb.org/anthology/P/P17/P17-1059.pdf) is another paper in the interesting-task category: the proposed model is an extension of the LSTM model (what else?) that *"... **customize[s] the degree of emotional content in generated sentences** through an additional design parameter."* This should come in handy for various dialogue systems that are deployed in affective scenarios - imagine your flight booking system actually reacting empathically to your distress at paying exorbitant luggage fees. We still have some way to go though, as the example sentences generated by the authors' model (with conditioning of the affect category) shows:
<br><img src="../img/P17-1059.png" width="650px">

* One of the - to me - fascinating areas of ACL are the **language construction and language evolution** studies. [*Naturalizing a Programming Language via Interactive Learning*](http://aclweb.org/anthology/P/P17/P17-1086.pdf) from Stanford falls into this category. The authors built [`Voxelturn`](http://www.voxelurn.com/#/about), a command-line system to create voxel structures such as these one:
<br><img src="../img/P17-1086-1.png" width="350px"><br>
and starting from a core programming language, allowed their users to "naturalize" the language by defining an alternative, more natural syntax. The experiment ran on MTurk (*Why do I never get to see those interesting HITS?*) and within a few days their user community had turned the core programming language into a high-level language.


* [*TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension*](http://aclweb.org/anthology/P/P17/P17-1147.pdf) introduces [`TriviaQA`](http://nlp.cs.washington.edu/triviaqa/index.html), a **question-answer-evidence dataset** with  with more than 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence
documents (from the Web and Wikipedia), six per question on average. The paper also contains a shout-out to TREC and the TREC-8 Question Answering track which was already released in the year 2000:
<br><img src="../img/P17-1147.png" width="650px"><br>
A first analysis by the authors indicates that this is a considerably more difficult dataset than [SQUAD](https://rajpurkar.github.io/SQuAD-explorer/), the Stanford Question Answering Dataset, currently one of the main benchmarks for reading comprehension. Some of `TriviaQA`'s questions remind me of the old [A Google A Day](https://en.wikipedia.org/wiki/A_Google_A_Day) quizzes, e.g. *A mill in Woodbridge, Suffolk, England, built in the 12th century, reconstructed in 1792, further restored in 2010 and currently in full working order is a early English example of a mill powered by what?*.

* [*Reading Wikipedia to Answer Open-Domain Questions*](http://aclweb.org/anthology/P/P17/P17-1171.pdf) comes out of Facebook's AI research lab and attempts to answer any factoid question with text snippet from a Wikipedia article. One of the few papers at ACL that includes an IR component (retrieving the Wikipedia articles relevant to a question) as part of their pipeline.
