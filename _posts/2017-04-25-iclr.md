---
layout: post
title: ICRL 2017 (Days 1 and 2)
---

Deep learning has been one of *the* dominant themes in the computer vision and natural language processing for the past few years. In *information retrieval* (my field), things are moving more slowly, we are still experimenting with word embeddings and are only beginning to adapt the (by now) classic CNN and RNN architectures to our needs. One reason for this is the lack of good data sources in particular in academia: existing query logs are either very old (the [infamous](https://en.wikipedia.org/wiki/AOL_search_data_leak) AOL query log is from 2006) or very small. And "small" does not go together well with deep networks. However, things are moving; last year's ACM SIGIR conference (one of the major information retrieval conferences) saw the inception of the first [Neural Information Retrieval Workshop](https://www.microsoft.com/en-us/research/event/neuir2016/) and [Chris Manning](https://nlp.stanford.edu/manning/) during his SIGIR keynote urged us all to get on board the deep net train (or else it will leave without us).

I have been interested in deep learning for quite some time and when this year's [International Conference on Learning Representations](http://iclr.cc) (or ICLR for short) was announced to be happening in France, and thus pretty close to my home, I wanted to be there. And indeed, here I am. The rest of the post contains my impressions of the conference and papers I (as an IR researcher) found interesting.

The conference is dominated by Google/Google Brain/DeepMind and Montreal - it is hard to find papers where none of the authors has an affiliation to one of those organisations. Dominant topics are [GANs](https://en.wikipedia.org/wiki/Generative_adversarial_networks), program generation, scalability issues, compression, reinforcement learning and (oddly to me) textures. The conference runs an interesting submission process: all submissions are *public* at submission time and *non-blind*. Reviewers (who are anonymous) and chairs also perform the review in public and anyone who is interested can comment on any submission at any time. Authors are invited to change their submissions in response to reviewer comments. Authors (anyone really) can also comment on the reviews. And even though the conference is in full swing, the whole process is still publicly available at the [OpenReview website](https://openreview.net/group?id=ICLR.cc/2017/conference) (it may take 1-2 minutes to load on a slow machine). What should also be mentioned is the fact that many authors concurrently with the submission (or before) post their work on arxiv because in deep learning weeks or even days count to claim ownership of an idea. All of this [can lead to pretty heated arguments - in public - between authors and reviewers](https://openreview.net/forum?id=BkjLkSqxg&noteId=BkjLkSqxg). Submissions that are rejected from the conference track may be accepted in the [workshop track](https://openreview.net/group?id=ICLR.cc/2017/workshop). 

The following list contains a rundown of the papers in both the workshop and conference track, presented in Days 1 and 2 (I'll add Day 3 later). Personally, I found the workshop track papers more accessible, some of the conference track papers where way over my head. And once more, these are works I selected from my academic IR perspective.

1. [Diet Networks: Thin Parameters for Fat Genomics](https://openreview.net/forum?id=Sk-oDY9ge&noteId=Sk-oDY9ge) describes an approach to cutting down on the number of parameters in deep nets for use cases where the number of input features is magnitudes larger than the number of training examples. Insufficient number of training examples is something any IR person should be able to relate to! 

2. [Pruning Filters for Efficient ConvNets](https://openreview.net/forum?id=rJqFGTslg&noteId=rJqFGTslg) is one of quite a few papers that propose clever techniques to reduce the computational costs of deep nets. 

3. [Learning Graphical State Transitions](https://openreview.net/forum?id=HJ0NvFzxl&noteId=HJ0NvFzxl) was one of the very few oral presentations - the vast majority of papers were "only" presented as posters. It is written by a sole author (Daniel Johnson) and on top of that the sole author is an undergraduate student! Amazing. The proposed model is able to construct and modify graph structures based on textual input (and use the graphs to generate textual output); the program chairs called the model ``quite complex'' in their acceptance decision and yes, the paper looks the part. The idea works extremely well on the [bAbI](https://research.fb.com/downloads/babi/) tasks. I am sure at least some IR tasks can also benefit from such structured intermediate representation.

4. [Snapshot Ensembles: Train 1, Get M for Free](https://openreview.net/forum?id=BJYwwY9ll&noteId=BJYwwY9ll): anybody who has ever participated in a [Kaggle competition](https://www.kaggle.com/competitions) or read [Kaggle's winners' interviews](http://blog.kaggle.com/category/winners-interviews/) will inevitably have heard all about ensembles and how useful they are to build robust and high quality machine learners. In deep learning ensembles are not yet common, due to the high computational costs in the training of a single deep net. This paper shows a clever trick of how to arrive at ensembles of deep nets without additional training costs! And as one could expect those ensembles are doing better than single nets.

5. [Learning End-to-End Goal-Oriented Dialog](https://openreview.net/forum?id=S1Bb3D5gg&noteId=S1Bb3D5gg) is a neat contribution that alters the goalpost in dialogue systems. In the words of the pc chairs: "Most dialog systems are based on chit-chat models. This paper explores goal-directed conversations, such as those that arise in booking a restaurant. While the methodology is rather thin, this is not the main focus of the paper. The authors provide creative evaluation protocols, and datasets."

6. [Efficient Vector Representation for Documents through Corruption](https://openreview.net/forum?id=B1Igu2ogg&noteId=B1Igu2ogg) deals with some of the major limitations (lack of efficiency) of doc2vec; the proposed method is not only faster but also works better than doc2vec. 

7. [Neural Architecture Search with Reinforcement Learning](https://openreview.net/forum?id=r1Ue8Hcxg&noteId=r1Ue8Hcxg): from time to time I happen to review papers at IR conferences that use deep nets. More often than not, I do not see any arguments for the chosen deep net architecture and it seems the net just appeared out of thin air. Now this paper could be an answer to engineering good architectures. In the words of the authors: "In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy."

8. [Dataset Augmentation in Feature Space](https://openreview.net/forum?id=HyaF53XYx&noteId=HyaF53XYx): as training deep nets requires a large amount of data, the image community has quickly found that creating copies of a training image by cropping/scaling/rotating it works well. The same cannot be said about the "text community" - what is the equivalent of rotating an image in the text domain? We don't know. According to this paper, we don't actually need to know: instead, we should create additional training items by performing simple transformations in the feature space (instead of the input space). Maybe a way for the IR community to augment our sadly looking sets of 50+ queries?

9. [Towards an automatic Turing test: Learning to evaluate dialogue responses](https://openreview.net/forum?id=Sk7c3yVYg&noteId=Sk7c3yVYg) is an interesting proposal for replacing a humanly interpretable evaluation metric (BLEU in this case) with a deep net for the case of dialogue evaluations. While I don't like an evaluation metric that is a black box, the authors' unarguably show that their net correlates much better with human judgments than BLEU. This is one of the few NLP works in the conference, I wonder how this would have been received at EMNLP or ACL.

10. 
