---
layout: post
title: ACL 2016
---

In this post, I am collecting the most interesting papers I came across when attending
the [2016 Association for Computational Linguistics conference](http://acl2016.org/). 
Nowadays, a lot of computational linguistics is *deep* and correspondingly
there is a lot of interest from industry to hear what is going on and canvas the scene for potential employees. 
This interest was reflected in the number of participants: more than 1600 people had signed
up. A large number of submissions also meant a large number of accepted papers and subsequently
the program ended up with up to SEVEN parallel sessions.
Overall, 231 long papers were accepted as well as 97 short papers and 28 system demonstrations. 

The proceedings are [available online](http://aclweb.org/anthology/P/P16/). Be prepared to spend
a week browsing through them.

The papers I list here are those that I personally found most insightful.

1. Have you ever trained word embeddings yourself (using `word2vec` or `GloVe` or ...)? Have you ever thought about the **order** in which you supply the training data? Does the order matter? As shown in [*Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning*](http://aclweb.org/anthology/P/P16/P16-1013.pdf) the order does matter (starting the training with simple items before complex ones works quite well) and has some impact when employing the learnt embeddings in a downstream NLP task. The paper asks an interesting question, takes an interesting approach to answer it (curriculum learning!) and provides all sorts of insightful analyses.

2. In [*Query Expansion with Locally-Trained Word Embeddings*](http://aclweb.org/anthology/P/P16/P16-1035.pdf) Diaz et al. show that in the ad hoc retrieval setting, drawing terms for query expansion from **globally trained** embeddings (i.e. those readily available pre-trained models as listed [here](https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models)) performs worse than drawing terms from small, **locally trained** embeddings. And here *small* really means small: the embeddings are trained on 1000 documents; *locally* trained refers to the fact that for each query, the training documents are probabilistically drawn from the set of documents with a high query likelihood score. The good news for the global embeddings crowd though is, that both *local* and *global* embeddings improve over the query likelihood baseline across all three evaluated corpora.

3. 





