---
layout: post
title: ACL 2016
---

In this post, I am collecting the most interesting papers I came across when attending
the [2016 Association for Computational Linguistics conference](http://acl2016.org/). 
Nowadays, a lot of computational linguistics is *deep* and correspondingly
there is a lot of interest from industry to hear what is going on and canvas the scene for potential employees. 
This interest was reflected in the number of participants: more than 1600 people had signed
up. A large number of submissions also meant a large number of accepted papers and subsequently
the program ended up with up to SEVEN parallel sessions.
Overall, 231 long papers were accepted as well as 97 short papers and 28 system demonstrations. 

The proceedings are [available online](http://aclweb.org/anthology/P/P16/). Be prepared to spend
a week browsing through them and repeatedly work through results such as this one:

<img src="../img/acl-example.png" width="600px">

A lot of deep learning (LSTMs are the new bag-of-words, at least in NLP) with usually one or two non-deep baselines.

The papers I list here are those that I personally found most insightful.

1. Have you ever trained word embeddings yourself (using `word2vec` or `GloVe` or ...)? Have you ever thought about the **order** in which you supply the training data? Does the order matter? As shown in [*Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning*](http://aclweb.org/anthology/P/P16/P16-1013.pdf) the order does matter (starting the training with simple items before complex ones works quite well) and has some impact when employing the learnt embeddings in a downstream NLP task. The paper asks an interesting question, takes an interesting approach to answer it and provides all sorts of insightful analyses.

2. The idea of *curriculum learning* (``inspired by the way humans acquire knowledge and skills: by mastering simple concepts first, and progressing through information with increasing difficulty to grasp more complex topics'') is also the starting point for the following paper: [*Easy Questions First? A Case Study on Curriculum Learning for Question Answering*](http://aclweb.org/anthology/P/P16/P16-1043.pdf). The authors show specifically for the task of question answering that good heuristics go a long way and choosing the *right* sequence of training examples can lead to substantial improvements over an uninformed baseline that receives training examples in random order.


3. In [*Query Expansion with Locally-Trained Word Embeddings*](http://aclweb.org/anthology/P/P16/P16-1035.pdf) Diaz et al. show that in the ad hoc retrieval setting, drawing terms for query expansion from **globally trained** embeddings (i.e. those readily available pre-trained models as listed [here](https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models)) performs worse than drawing terms from small, **locally trained** embeddings. And here *small* really means small: the embeddings are trained on 1000 documents; *locally* trained refers to the fact that for each query, the training documents are probabilistically drawn from the set of documents with a high query likelihood score. The good news for the global embeddings crowd though is, that both *local* and *global* embeddings improve over the query likelihood baseline across all three evaluated corpora.

4. [*DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents*](http://aclweb.org/anthology/P/P16/P16-1049.pdf) is one of the few works at the conference that use IR technology to solve an NLP task: automated short text conversations. Modern conversational agents usually use one of two approaches to reply to a user's utterance: (1) finding the best utterance-response pair from the training data, or (2) generating a response using deep learning sentence generation techniques. The authors explore a third approach: (3) using learning to rank in order to rank potential response sentences from a set of documents in reply to an utterance. While it sounds rather straight-forward, it is not: the feature engineering going on in the paper is tremendous (and it is of course deep). Based on the description I'd imagine a language generation model to be easier to set up, train and tune than this. On the bright side, the feature engineering work was not in vain: the evaluation showed *DocChat* to be state-of-the-art.

5. 





