---
layout: post
title: ACL 2016
---

In this post, I have collected the most interesting papers I came across when attending
the [2016 Association for Computational Linguistics conference](http://acl2016.org/).
Nowadays, a lot of computational linguistics is *deep* and correspondingly
there is a lot of interest from industry to hear what is going on and canvas the scene for potential employees. 
This interest was reflected in the number of participants: more than 1600 people had signed
up. A large number of submissions also meant a large number of accepted papers and subsequently
the program ended up with up to SEVEN parallel sessions.
Overall, 231 long papers were accepted as well as 97 short papers and 28 system demonstrations. 

The proceedings are [available online](http://aclweb.org/anthology/P/P16/). Be prepared to spend
a week browsing through them and repeatedly work through results such as this one, which shows a lot of deep learning (LSTMs are the new bag-of-words, at least in NLP) with usually one or two non-deep baselines:

<img src="../img/acl-example.png" width="350px">

Visualizations of deep nets have also become more refined, though I still have a hard time working through them:

<img src="../img/acl-example_2.png" width="350px">

The list is only based on the long papers, I did not have time yet to work through the short ones.

1. Have you ever trained word embeddings yourself (using `word2vec` or `GloVe` or ...)? Have you ever thought about the **order** in which you supply the training data? Does the order matter? As shown in [*Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning*](http://aclweb.org/anthology/P/P16/P16-1013.pdf) the order does matter (starting the training with simple items before complex ones works quite well) and has some impact when employing the learnt embeddings in a downstream NLP task. The paper asks an interesting question, takes an interesting approach to answer it and provides all sorts of insightful analyses.

2. The idea of *curriculum learning* (``inspired by the way humans acquire knowledge and skills: by mastering simple concepts first, and progressing through information with increasing difficulty to grasp more complex topics'') is also the starting point for the following paper: [*Easy Questions First? A Case Study on Curriculum Learning for Question Answering*](http://aclweb.org/anthology/P/P16/P16-1043.pdf). The authors show specifically for the task of question answering that good heuristics go a long way and choosing the *right* sequence of training examples can lead to substantial improvements over an uninformed baseline that receives training examples in random order.

3. In [*Query Expansion with Locally-Trained Word Embeddings*](http://aclweb.org/anthology/P/P16/P16-1035.pdf) Diaz et al. show that in the ad hoc retrieval setting, drawing terms for query expansion from **globally trained** embeddings (i.e. those readily available pre-trained models as listed [here](https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models)) performs worse than drawing terms from small, **locally trained** embeddings. And here *small* really means small: the embeddings are trained on 1000 documents; *locally* trained refers to the fact that for each query, the training documents are probabilistically drawn from the set of documents with a high query likelihood score. The good news for the global embeddings crowd though is, that both *local* and *global* embeddings improve over the query likelihood baseline across all three evaluated corpora.

4. [*DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents*](http://aclweb.org/anthology/P/P16/P16-1049.pdf) is one of the few works at the conference that use IR technology to solve an NLP task: **automated short text conversations**. Modern conversational agents usually use one of two approaches to reply to a user's utterance: (1) finding the best utterance-response pair from the training data, or (2) generating a response using deep learning sentence generation techniques. The authors explore a third approach: (3) using learning to rank to rank potential response sentences from a set of documents in reply to an utterance. While it sounds rather straight-forward, it is not: the feature engineering going on in the paper is tremendous (and it is of course deep). Based on the description I'd imagine a language generation model to be easier to set up, train and tune than this. On the bright side, the feature engineering work was not in vain: the evaluation showed *DocChat* to be state-of-the-art.

5. An interesting task was proposed in [*How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions*](http://aclweb.org/anthology/P/P16/P16-1055.pdf). Given a statement such as ``Water is flowing into Taihu lake at a rate of 150 cubic meters per second`` the authors' system automatically generates a perspective to make the scale of the numeric mention more understandable, e.g. ``That's about how much water would flow from a tap left on for a week``. The paper describes the entire pipeline, starting with dissecting the task into several componenets and ending with an exhaustive human-centred evaluation. 

6. [*Latent Predictor Networks for Code Generation*](http://aclweb.org/anthology/P/P16/P16-1057.pdf). This rather boring title hides the fact that the authors used a source code dataset generated from **Magic the Gathering** and **Hearthstone** game implementations. It deserves a mention just for that fact.
 
7. [*A short proof that O_2 is an MCFL*](http://aclweb.org/anthology/P/P16/P16-1106.pdf) - a ten page proof that I did not understand at all; the only of its kind in the proceedings. 

8. [*Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus*](http://aclweb.org/anthology/P/P16/P16-1056.pdf). As the title implies, the paper describes by far the largest publicly available natural language question-answering corpus and its generation from Freebase facts (not hard to guess that neural machine translation makes a prominent appearance here). The conducted human evaluation found the generated questions to be undistinguishable from real human-generated questions. The corpus is available for download [here](http://agarciaduran.org/).

9. Automatically generating concept maps (or concept graphs) for learners to help them work through learning material has been high on the wish list for our MOOC research for a while. The paper [*Modeling Concept Dependencies in a Scientific Corpus*](http://aclweb.org/anthology/P/P16/P16-1082.pdf) tackles this problem through the generation of concept graphs from unstructured documents. Given a query `X`, the generated concept graphs answers the question ``What do I need to know before I start reading about X?``

10. [*Siamese CBOW: Optimizing Word Embeddings for Sentence Representations*](http://aclweb.org/anthology/P/P16/P16-1089.pdf)

11. [*Learning Word Meta-Embeddings*](http://aclweb.org/anthology/P/P16/P16-1128.pdf)

12. [*Generating Natural Questions About an Image*](http://aclweb.org/anthology/P/P16/P16-1170.pdf)

13. [*Strategies for Training Large Vocabulary Neural Language Models*](http://aclweb.org/anthology/P/P16/P16-1186.pdf)

14. [*Summarizing Source Code using a Neural Attention Model*](http://aclweb.org/anthology/P/P16/P16-1195.pdf)

15. [*Unravelling Names of Fictional Characters*](http://aclweb.org/anthology/P/P16/P16-1203.pdf)

16. [*Learning Language Games through Interaction*](http://aclweb.org/anthology/P/P16/P16-1224.pdf)






